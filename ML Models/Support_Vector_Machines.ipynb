{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZNoCm3gKqyJ8rJT1wagnz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MELRIAN1910/MachineLearning/blob/main/ML%20Models/Support_Vector_Machines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the model"
      ],
      "metadata": {
        "id": "7F3idnNUTjOk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh5dYvJTRyvP"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "#support vector machine classifier\n",
        "class svm_classifier():\n",
        "  #initiating the hyper parameters\n",
        "  def __init__(self, learning_rate, no_of_iterations, lambda_parameter):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.no_of_iterations = no_of_iterations\n",
        "    self.lambda_parameter = lambda_parameter\n",
        "\n",
        "  #fitting the dataset\n",
        "  def fit(self, X, Y):\n",
        "    #no of data points in the dataset\n",
        "    self.m, self.n = X.shape\n",
        "    #initiating weight value and bias value\n",
        "    self.w = np.zeros(self.n)\n",
        "    self.b = 0\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    #implementing gradient descent for optimization\n",
        "    for i in range(self.no_of_iterations):\n",
        "      self.update_weights()\n",
        "\n",
        "    #function for updating weight and bias\n",
        "    def update_weights(self):\n",
        "      #label encoding\n",
        "      y_label = np.where(self.Y <= 0, -1, 1)\n",
        "      #gradients (dw, db)\n",
        "      for index, x_i in enumerate(self.X):\n",
        "        condition = y_label[index] * (np.dot(x_i, self.w) - self.b) >= 1\n",
        "        if (condition == True):\n",
        "          dw = 2 * self.lambda_parameter * self.w\n",
        "          db = 0\n",
        "        else:\n",
        "          dw = 2 * self.lambda_parameter * self.w - np.dot(x_i, y_label[index])\n",
        "          db = y_label[index]\n",
        "        self.w = self.w - self.learning_rate * dw\n",
        "        self.b = self.b - self.learning_rate * db\n",
        "\n",
        "    #predict the label for a given input value\n",
        "    def predict(self, X):\n",
        "      output = np.dot(X, self.w) - self.b\n",
        "      predicted_labels = np.sign(output)\n",
        "      y_hat = np.where(predicted_labels <= -1, 0, 1)\n",
        "      return y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing the model"
      ],
      "metadata": {
        "id": "mOmhZo_KToPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import dependencies\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#data collection and processing\n",
        "diabetes_dataset = pd.read_csv('path')\n",
        "#first 5 data\n",
        "diabetes_dataset.head()\n",
        "#no of rows and columns\n",
        "diabetes_dataset.shape\n",
        "#statistical measures of the dataset\n",
        "diabetes_dataset.describe()\n",
        "#no of diabetes and no of non diabetes\n",
        "diabetes_dataset['Outcome'].value_counts()\n",
        "\n",
        "#seperating features and target\n",
        "features = diabetes_dataset.drop(columns = 'Outcome', axis = 1)\n",
        "target = diabetes_dataset['Outcome']\n",
        "print(features)\n",
        "print(target)\n",
        "\n",
        "#data standatrdization\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(features)\n",
        "standardized_data = scaler.transform(features)\n",
        "print(standardized_data)\n",
        "\n",
        "features = standardized_data\n",
        "target = diabetes_dataset['Outcome']\n",
        "print(features)\n",
        "print(target)\n",
        "\n",
        "#TRAIN TEST SPLIT\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size = 0.2, random_state = 2)\n",
        "print(features.shape, X_train.shape, X_test.shape)\n",
        "\n",
        "#Training SVM classifier\n",
        "classifier = svm_classifier(learning_rate = 0.001, no_of_iterations = 1000, lambda_parameter = 0.01)\n",
        "classifier.fit(X_train, Y_train)\n",
        "\n",
        "#model evaluation\n",
        "#accuracy score on training data\n",
        "X_train_prediction = classifier.predict(X_train)\n",
        "training_data_accuracy = accuracy_score(Y_train, X_train_prediction)\n",
        "print('Accuracy score on training data : ', training_data_accuracy)\n",
        "#accuracy on testing data\n",
        "X_test_prediction = classifier.predict(X_test)\n",
        "test_data_accuracy = accuracy_score(Y_test, X_test_prediction)\n",
        "print('Accuracy score on test data : ', test_data_accuracy)\n",
        "\n",
        "#building predictive system\n",
        "input_data = (5,166,72,19,175,25.8,0.587,51)\n",
        "#change the input data to numpy array\n",
        "input_data_as_numpy_array = np.asarray(input_data)\n",
        "#reshape the array as we are predicting for one instance\n",
        "input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n",
        "#standardizing input\n",
        "std_data = scaler.transform(input_data_reshaped)\n",
        "print(std_data)\n",
        "prediction = classifier.predict(std_data)\n",
        "print(prediction)\n",
        "if (prediction[0] == 0):\n",
        "  print('The person is not diabetic')\n",
        "else:\n",
        "  print('The person is diabetic')"
      ],
      "metadata": {
        "id": "yGrEezUfTtB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YiQzYnLDV8F7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the pre-built model"
      ],
      "metadata": {
        "id": "c1p0J2lfV9VO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import dependencies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from SVM import SVM_classifier\n",
        "\n",
        "#data collection and processing\n",
        "diabetes_dataset = pd.read_csv('path')\n",
        "#first 5 data\n",
        "diabetes_dataset.head()\n",
        "#no of rows and columns\n",
        "diabetes_dataset.shape\n",
        "#statistical measures of the dataset\n",
        "diabetes_dataset.describe()\n",
        "#no of diabetes and no of non diabetes\n",
        "diabetes_dataset['Outcome'].value_counts()\n",
        "\n",
        "#seperating features and target\n",
        "features = diabetes_dataset.drop(columns = 'Outcome', axis = 1)\n",
        "target = diabetes_dataset['Outcome']\n",
        "print(features)\n",
        "print(target)\n",
        "\n",
        "#data standatrdization\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(features)\n",
        "standardized_data = scaler.transform(features)\n",
        "print(standardized_data)\n",
        "\n",
        "features = standardized_data\n",
        "target = diabetes_dataset['Outcome']\n",
        "print(features)\n",
        "print(target)\n",
        "\n",
        "#TRAIN TEST SPLIT\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size = 0.2, random_state = 2)\n",
        "print(features.shape, X_train.shape, X_test.shape)\n",
        "\n",
        "#Training SVM classifier\n",
        "classifier = SVM_classifier(learning_rate=0.001, no_of_iterations=1000, lambda_parameter = 0.01)\n",
        "classifier.fit(X_train, Y_train)\n",
        "\n",
        "#model evaluation\n",
        "#accuracy score on training data\n",
        "X_train_prediction = classifier.predict(X_train)\n",
        "training_data_accuracy = accuracy_score(Y_train, X_train_prediction)\n",
        "print('Accuracy score on training data : ', training_data_accuracy)\n",
        "#accuracy on testing data\n",
        "X_test_prediction = classifier.predict(X_test)\n",
        "test_data_accuracy = accuracy_score(Y_test, X_test_prediction)\n",
        "print('Accuracy score on test data : ', test_data_accuracy)\n",
        "\n",
        "#building predictive system\n",
        "input_data = (5,166,72,19,175,25.8,0.587,51)\n",
        "#change the input data to numpy array\n",
        "input_data_as_numpy_array = np.asarray(input_data)\n",
        "#reshape the array as we are predicting for one instance\n",
        "input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n",
        "#standardizing input\n",
        "std_data = scaler.transform(input_data_reshaped)\n",
        "print(std_data)\n",
        "prediction = classifier.predict(std_data)\n",
        "print(prediction)\n",
        "if (prediction[0] == 0):\n",
        "  print('The person is not diabetic')\n",
        "else:\n",
        "  print('The person is diabetic')"
      ],
      "metadata": {
        "id": "VHRgpKEqWDx2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}