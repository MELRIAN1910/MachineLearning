{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIGfKkPVdd0DilhVSYPi9C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MELRIAN1910/MachineLearning/blob/main/ML%20Models/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the model"
      ],
      "metadata": {
        "id": "fZ9j6iJBDR5A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCOVAkQKA8VX"
      },
      "outputs": [],
      "source": [
        "#import Libraries\n",
        "import numpy as np\n",
        "\n",
        "#logistic Regression\n",
        "class LogisticRegression:\n",
        "  #declaring learning rate and number of iterations\n",
        "  def __init__(self, learning_rate, no_of_iterations):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.no_of_iterations = no_of_iterations\n",
        "\n",
        "  #fit function to train the model with dataset\n",
        "  def fit(self, X, Y):\n",
        "    #number of data points in the dataset\n",
        "    self.m, self.n = X.shape\n",
        "    #intiating weight and bias\n",
        "    self.w = np.zeros(self.n)\n",
        "    self.b = 0\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    #implementing gradient descent for optimization\n",
        "    for i in range(self.no_of_iterations):\n",
        "      self.update_weights()\n",
        "\n",
        "  #function to update weights and bias value\n",
        "  def update_weights(self):\n",
        "    #Y_hat formula (sigmoid function)\n",
        "    Y_hat = 1 / (1 + np.exp(-(self.X.dot(self.w) + self.b)))\n",
        "    #derivatives\n",
        "    dw = (1 / self.m) * np.dot(self.X.T, (Y_hat - self.Y))\n",
        "    db = (1 / self.m) * np.sum(Y_hat - self.Y)\n",
        "    #updating the weights and bias using gradient descent\n",
        "    self.w = self.w - self.learning_rate * dw\n",
        "    self.b = self.b - self.learning_rate * db\n",
        "\n",
        "  #sigmoid Equation and decision boundary\n",
        "  def predict(self, X):\n",
        "    Y_pred = 1 / (1 + np.exp(-(X.dot(self.w) + self.b)))\n",
        "    Y_pred = np.where(Y_pred > 0.5, 1, 0)\n",
        "    return Y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing the model"
      ],
      "metadata": {
        "id": "Z4pvunAmEXxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the dependencies\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Data collection and analysis\n",
        "#loading data set\n",
        "diabetes_dataset = pd.read_csv('paths')\n",
        "diabetes_dataset.head()\n",
        "diabetes_dataset.tail()\n",
        "diabetes_dataset.shape\n",
        "\n",
        "#getting statistical measures\n",
        "diabetes_dataset.describe()\n",
        "#count data numbers with each outcome\n",
        "diabetes_dataset['Outcome'].value_counts()\n",
        "diabetes_dataset.groupby('Outcome').mean()\n",
        "\n",
        "#separating the data and labels\n",
        "features = diabetes_dataset.drop(columns = 'Outcome', axis=1)\n",
        "target = diabetes_dataset['Outcome']\n",
        "print(features)\n",
        "print(target)\n",
        "\n",
        "#data Standardization\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(features)\n",
        "standardized_data = scaler.transform(features)\n",
        "print(standardized_data)\n",
        "features = standardized_data\n",
        "target = diabetes_dataset['Outcome']\n",
        "print(features)\n",
        "print(target)\n",
        "\n",
        "#train test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(features,target, test_size = 0.2, stratify=target, random_state=2)\n",
        "print(features.shape, X_train.shape, X_test.shape)\n",
        "\n",
        "#training the model\n",
        "classifier = LogisticRegression(learning_rate=0.01, no_of_iterations=1000)\n",
        "#training the logistic regression classifier\n",
        "classifier.fit(X_train, Y_train)\n",
        "\n",
        "#Model Evaluation\n",
        "#Accuracy score\n",
        "#accuracy on training data\n",
        "X_train_prediction = classifier.predict(X_train)\n",
        "training_data_accuracy = accuracy_score( Y_train, X_train_prediction)\n",
        "print('Accuracy score of the training data : ', training_data_accuracy)\n",
        "\n",
        "#make a predictive system\n",
        "input_data = (15,166,72,19,175,25.8,0.587,51)\n",
        "#change the input data to numpy array\n",
        "input_data_as_numpy_array = np.asarray(input_data)\n",
        "#reshape the array as we are predicting for one instance\n",
        "input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n",
        "#standardize the input data\n",
        "std_data = scaler.transform(input_data_reshaped)\n",
        "print(std_data)\n",
        "prediction = classifier.predict(std_data)\n",
        "print(prediction)\n",
        "\n",
        "if (prediction[0] == 0):\n",
        "  print('The person is not diabetic')\n",
        "else:\n",
        "  print('The person is diabetic')"
      ],
      "metadata": {
        "id": "UJVAPZFOE2Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement model with pre built model"
      ],
      "metadata": {
        "id": "Ehd4k3UaKIA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the dependencies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import Log_Reg\n",
        "\n",
        "#Data collection and analysis\n",
        "#loading data set\n",
        "diabetes_dataset = pd.read_csv('paths')\n",
        "diabetes_dataset.head()\n",
        "diabetes_dataset.tail()\n",
        "diabetes_dataset.shape\n",
        "\n",
        "#getting statistical measures\n",
        "diabetes_dataset.describe()\n",
        "#count data numbers with each outcome\n",
        "diabetes_dataset['Outcome'].value_counts()\n",
        "diabetes_dataset.groupby('Outcome').mean()\n",
        "\n",
        "#separating the data and labels\n",
        "features = diabetes_dataset.drop(columns = 'Outcome', axis=1)\n",
        "target = diabetes_dataset['Outcome']\n",
        "print(features)\n",
        "print(target)\n",
        "\n",
        "#data Standardization\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(features)\n",
        "standardized_data = scaler.transform(features)\n",
        "print(standardized_data)\n",
        "features = standardized_data\n",
        "target = diabetes_dataset['Outcome']\n",
        "print(features)\n",
        "print(target)\n",
        "\n",
        "#train test split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(features,target, test_size = 0.2, stratify=target, random_state=2)\n",
        "print(features.shape, X_train.shape, X_test.shape)\n",
        "\n",
        "#training the model\n",
        "classifier = Log_Reg.Logistic_Regression(learning_rate=0.01, no_of_iterations=1000)\n",
        "#training the logistic regression classifier\n",
        "classifier.fit(X_train, Y_train)\n",
        "\n",
        "#Model Evaluation\n",
        "#Accuracy score\n",
        "#accuracy on training data\n",
        "X_train_prediction = classifier.predict(X_train)\n",
        "training_data_accuracy = accuracy_score( Y_train, X_train_prediction)\n",
        "print('Accuracy score of the training data : ', training_data_accuracy)\n",
        "\n",
        "#make a predictive system\n",
        "input_data = (15,166,72,19,175,25.8,0.587,51)\n",
        "#change the input data to numpy array\n",
        "input_data_as_numpy_array = np.asarray(input_data)\n",
        "#reshape the array as we are predicting for one instance\n",
        "input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n",
        "#standardize the input data\n",
        "std_data = scaler.transform(input_data_reshaped)\n",
        "print(std_data)\n",
        "prediction = classifier.predict(std_data)\n",
        "print(prediction)\n",
        "\n",
        "if (prediction[0] == 0):\n",
        "  print('The person is not diabetic')\n",
        "else:\n",
        "  print('The person is diabetic')"
      ],
      "metadata": {
        "id": "X1jlJD4MKO0s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}